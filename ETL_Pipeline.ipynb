{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore.session\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, col, split, udf\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pyspark.mllib.feature import StandardScaler, StandardScalerModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow 2 = INFO and WARNING messages are not printed\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ceecf",
   "metadata": {},
   "source": [
    "<h1>Set UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38425f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "    '--packages com.amazonaws:aws-java-sdk-pom:1.12.249,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\\\n",
    "       .setAppName(\"ETL Pipeline\")\\\n",
    "       .setMaster(\"local[2]\")\\\n",
    "       .set(\"spark.hadoop.fs.s3a.access.key\", credentials.access_key)\\\n",
    "       .set(\"spark.hadoop.fs.s3a.secret.key\", credentials.secret_key)\\\n",
    "       .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58a0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf).getOrCreate()\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917fcbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6621c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_bytes_images = sc.binaryFiles(\"s3a://oc-project-8-bucket/Sub_S3/**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a55507",
   "metadata": {},
   "source": [
    "<h1>Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575d8b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd_array_image = rdd_bytes_images.map(lambda x: np.asarray(Image.open(io.BytesIO(x[1])).resize((224,224))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights=\"imagenet\", \n",
    "                     pooling='max', \n",
    "                     include_top=False, \n",
    "                     input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "model.compile()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f8fd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rdd_extracted_features = rdd_array_image.map(lambda x : model.predict(np.array([x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bce1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_extracted_features.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe6ad2",
   "metadata": {},
   "source": [
    "<h1>Dimentionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5380498",
   "metadata": {},
   "source": [
    "<h6>N.B : As the feature extraction space is on the same scale, standardisation is not necessary to perform a proper PCA. This allows us to not trigger the standardisation action and then imporve performance of our Pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = RowMatrix(rdd_extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e45a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca = matrix.computePrincipalComponents(137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba072578",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducted_matrix = matrix.multiply(pca).rows.map(lambda x : x.toArray().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14d682",
   "metadata": {},
   "source": [
    "<h1>CSV Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b8a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8d931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv = reducted_matrix.toDF().toPandas().to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a06f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'oc-project-8-pca-csv'\n",
    "\n",
    "# boto3.client('s3')\\\n",
    "#      .create_bucket(Bucket=bucket_name,\n",
    "#                     CreateBucketConfiguration={'LocationConstraint':'eu-west-3'})\n",
    "\n",
    "boto_config = TransferConfig(multipart_threshold=1024 * 25, max_concurrency=10, \n",
    "                             multipart_chunksize=1024 * 25, use_threads=True)\n",
    "\n",
    "boto3.resource('s3').meta.client.upload_fileobj(Fileobj=io.BytesIO(str.encode(csv)), \n",
    "                                                Bucket=bucket_name, \n",
    "                                                Key='pca.csv',\n",
    "                                                ExtraArgs={'ContentType':'text/csv', \n",
    "                                                           'ACL':'private'},\n",
    "                                                Config=boto_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

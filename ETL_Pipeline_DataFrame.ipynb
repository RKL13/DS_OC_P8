{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2dcb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore.session\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, col, split, udf, \\\n",
    "                                  split, monotonically_increasing_id\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import StringType, ArrayType, DoubleType\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pyspark.mllib.feature import StandardScaler, StandardScalerModel\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.feature import StandardScaler, PCA\n",
    "from datetime import datetime\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc88586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow 2 = INFO and WARNING messages are not printed\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f9083",
   "metadata": {},
   "source": [
    "<h1>Set UP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a2eb1",
   "metadata": {},
   "source": [
    "<h6>Pyspark Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves given aws credentials \n",
    "\n",
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "# Applies necessary packages to pyspark to work\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "    '--packages com.amazonaws:aws-java-sdk-pom:1.12.249,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff96961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a Boto3 Session\n",
    "\n",
    "boto_session = boto3.Session( \n",
    "         aws_access_key_id=credentials.access_key, \n",
    "         aws_secret_access_key=credentials.secret_key)\n",
    "\n",
    "s3 = boto_session.resource('s3')\n",
    "\n",
    "bucket = s3.Bucket('oc-project-8-pca-csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993878dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder\\\n",
    "    .appName(\"ETL Pipeline\")\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", credentials.access_key)\\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", credentials.secret_key)\\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad921fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the SparkSession object\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41ca74",
   "metadata": {},
   "source": [
    "<h6>Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the S3 bucket path of images\n",
    "\n",
    "s3_object_path = \"s3a://oc-project-8-bucket/Sub_S3/**\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91090394",
   "metadata": {},
   "source": [
    "<h6> Tensorflow Model (VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447b4ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Defines VGG16 model and sets parameters as non trainable\n",
    "# NB : the model implements a max poolings as the ultimate layer \n",
    "# to use the model as a feature extraction \"tool\"\n",
    "\n",
    "model = VGG16(weights=\"imagenet\",\n",
    "                     pooling='max', \n",
    "                     include_top=False,\n",
    "                     input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "model.compile()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61671639",
   "metadata": {},
   "source": [
    "<h6>UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed81ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines several UDFs functions\n",
    "\n",
    "# Functions\n",
    "\n",
    "def feature_extract(bytes_type):\n",
    "    \n",
    "    \"\"\"Transforms a given picture in a 224*224 matrix\"\"\"\n",
    "\n",
    "    lambda_list = np.asarray(Image.open(io.BytesIO(bytes_type)).resize((224,224)))\n",
    "    \n",
    "    vectorized = model.predict(np.array([lambda_list])).tolist()[0]\n",
    "                             \n",
    "    return vectorized\n",
    "\n",
    "def extract_label(path):\n",
    "    \n",
    "    \"\"\"Extracts fruit label from the image path\"\"\"\n",
    "    \n",
    "    splitted_list = re.split('/', path)\n",
    "    \n",
    "    return splitted_list[4]\n",
    "\n",
    "# UDFs\n",
    "\n",
    "feature_extract_UDF = udf(lambda x: feature_extract(x), ArrayType(DoubleType()))\n",
    "\n",
    "to_vector_UDF = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "\n",
    "label_extract_UDF = udf(lambda path: extract_label(path), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca09e0",
   "metadata": {},
   "source": [
    "<h1>Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9adba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retreives images from S3 bucket\n",
    "\n",
    "df = spark.read.format(\"binaryFile\").load(s3_object_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the features from the images\n",
    "\n",
    "feature_extracted_df = df.select(feature_extract_UDF(df.content).alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the rows of the dataframe in dense vectors for further pyspark ml usage \n",
    "\n",
    "vector_df = feature_extracted_df.select(to_vector_UDF('features').alias('features'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b001d35",
   "metadata": {},
   "source": [
    "<h1>Dimentionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50488580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up a ML pipeline with Standardisation and PCA (n_compoenents = 150)\n",
    "\n",
    "n_components = 150\n",
    "\n",
    "std = StandardScaler(inputCol=\"features\", outputCol=\"scaled\")\n",
    "pca = PCA(inputCol=\"scaled\", outputCol=\"pca\").setK(n_components)\n",
    "stages = [std, pca]\n",
    "\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e0a7f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Applies the ml pipeline\n",
    "\n",
    "pca_df = pipeline.fit(vector_df).transform(vector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1aff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms the single output column in as many column as there are Pcs Dataframe\n",
    "\n",
    "pca_df_multiple_columns = pca_df.withColumn(\"pc\", vector_to_array(\"pca\"))\\\n",
    "                          .select([col(\"pc\")[i] for i in range(n_components)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the labels from the df\n",
    "\n",
    "label_df = df.select(label_extract_UDF(df.path).alias(\"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives ids to both dataframes for further join\n",
    "\n",
    "pca_df_multiple_columns_id = \\\n",
    "    pca_df_multiple_columns.withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "label_df_id = label_df.withColumn('id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3936f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins PCs dataframe with relative labels\n",
    "\n",
    "labeled_pca_df = label_df_id.join(pca_df_multiple_columns_id, on=['id'], how='inner').drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write in a bucket the dataframe as a csv\n",
    "# N.B : coalesce(1) is implemented to have one csv as an output\n",
    "\n",
    "labeled_pca_df.coalesce(1)\\\n",
    "           .write.mode('overwrite')\\\n",
    "           .save(\"s3a://oc-project-8-pca-csv/run/\", format='csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa138e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves the name of the created csv file (as pyspark doesn't have a way to name its written files)\n",
    "\n",
    "for bucket_object in bucket.objects.all():\n",
    "    if re.compile('^run/part').match(bucket_object.key):\n",
    "        pca_csv_file_name = bucket_object.key\n",
    "\n",
    "# Copy the csv file under a constant name for easier retrieval\n",
    "\n",
    "s3.Object('oc-project-8-pca-csv','pca_csv.csv')\\\n",
    ".copy_from(CopySource='oc-project-8-pca-csv/{}'.format(pca_csv_file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

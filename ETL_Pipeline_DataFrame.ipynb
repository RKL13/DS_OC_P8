{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2dcb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore.session\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, col, split, udf, \\\n",
    "                                  split, monotonically_increasing_id\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import StringType, ArrayType, DoubleType\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pyspark.mllib.feature import StandardScaler, StandardScalerModel\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.feature import StandardScaler, PCA\n",
    "from datetime import datetime\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc88586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow 2 = INFO and WARNING messages are not printed\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f9083",
   "metadata": {},
   "source": [
    "<h1>Set UP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a2eb1",
   "metadata": {},
   "source": [
    "<h6>Pyspark Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "    '--packages com.amazonaws:aws-java-sdk-pom:1.12.249,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993878dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder\\\n",
    "    .appName(\"ETL Pipeline\")\\\n",
    "    .master(\"local[2]\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", credentials.access_key)\\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", credentials.secret_key)\\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad921fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41ca74",
   "metadata": {},
   "source": [
    "<h6>Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_object_path = \"s3a://oc-project-8-bucket/Sub_S3/**\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91090394",
   "metadata": {},
   "source": [
    "<h6> Tensorflow Model (VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447b4ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = VGG16(weights=\"imagenet\",\n",
    "                     pooling='max', \n",
    "                     include_top=False,\n",
    "                     input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "model.compile()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61671639",
   "metadata": {},
   "source": [
    "<h6>UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed81ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def feature_extract(bytes_type):\n",
    "\n",
    "    lambda_list = np.asarray(Image.open(io.BytesIO(bytes_type)).resize((224,224)))\n",
    "    \n",
    "    vectorized = model.predict(np.array([lambda_list])).tolist()[0]\n",
    "                             \n",
    "    return vectorized\n",
    "\n",
    "def extract_label(path):\n",
    "    \n",
    "    splitted_list = re.split('/', path)\n",
    "    \n",
    "    return splitted_list[4]\n",
    "\n",
    "# UDFs\n",
    "\n",
    "feature_extract_UDF = udf(lambda x: feature_extract(x), ArrayType(DoubleType()))\n",
    "\n",
    "to_vector_UDF = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "\n",
    "label_extract_UDF = udf(lambda path: extract_label(path), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca09e0",
   "metadata": {},
   "source": [
    "<h1>Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9adba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"binaryFile\").load(s3_object_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extracted_df = df.select(feature_extract_UDF(df.content).alias(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df = feature_extracted_df.select(to_vector_UDF('features').alias('features'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b001d35",
   "metadata": {},
   "source": [
    "<h1>Dimentionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50488580",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 137\n",
    "\n",
    "std = StandardScaler(inputCol=\"features\", outputCol=\"scaled\")\n",
    "pca = PCA(inputCol=\"scaled\", outputCol=\"pca\").setK(n_components)\n",
    "stages = [std, pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e0a7f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_df = pipeline.fit(vector_df).transform(vector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1aff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_multiple_columns = pca_df.withColumn(\"pc\", vector_to_array(\"pca\"))\\\n",
    "                          .select([col(\"pc\")[i] for i in range(n_components)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eb0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = df.select(label_extract_UDF(df.path).alias(\"labels\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851f5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df_multiple_columns_id = \\\n",
    "    pca_df_multiple_columns.withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "label_df_id = label_df.withColumn('id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3936f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_pca_df = label_df_id.join(pca_df_multiple_columns_id, on=['id'], how='inner').drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9cc2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = labeled_pca_df.toPandas().to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325acd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'oc-project-8-pca-csv'\n",
    "\n",
    "# boto3.client('s3')\\\n",
    "#      .create_bucket(Bucket=bucket_name,\n",
    "#                     CreateBucketConfiguration={'LocationConstraint':'eu-west-3'})\n",
    "\n",
    "boto_config = TransferConfig(multipart_threshold=1024 * 25, max_concurrency=10, \n",
    "                             multipart_chunksize=1024 * 25, use_threads=True)\n",
    "\n",
    "boto3.resource('s3').meta.client.upload_fileobj(Fileobj=io.BytesIO(str.encode(csv)), \n",
    "                                                Bucket=bucket_name, \n",
    "                                                Key='pca.csv',\n",
    "                                                ExtraArgs={'ContentType':'text/csv', \n",
    "                                                           'ACL':'private'},\n",
    "                                                Config=boto_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2dcb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from IPython.display import display\n",
    "import boto3\n",
    "import botocore.session\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, col, split, udf, \\\n",
    "                                  split, monotonically_increasing_id\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.types import StringType, ArrayType, DoubleType\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pyspark.mllib.feature import StandardScaler, StandardScalerModel\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.ml.feature import StandardScaler, PCA\n",
    "from datetime import datetime\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import re\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726ec25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the Ipython options\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc88586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow 2 = INFO and WARNING messages are not printed\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1f9083",
   "metadata": {},
   "source": [
    "<h1>Set UP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd1d4e",
   "metadata": {},
   "source": [
    "<h6>Spark Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = botocore.session.get_session()\n",
    "credentials = session.get_credentials()\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "    '--packages com.amazonaws:aws-java-sdk-pom:1.12.249,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91090394",
   "metadata": {},
   "source": [
    "<h6> Tensorflow Model (VGG16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c447b4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights=\"imagenet\",\n",
    "                     pooling='max', \n",
    "                     include_top=False,\n",
    "                     input_shape=(224, 224, 3))\n",
    "\n",
    "for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "model.compile()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61671639",
   "metadata": {},
   "source": [
    "<h6>UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed81ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def feature_extract(bytes_type):\n",
    "\n",
    "    lambda_list = np.asarray(Image.open(io.BytesIO(bytes_type)).resize((224,224)))\n",
    "    \n",
    "    vectorized = model.predict(np.array([lambda_list])).tolist()[0]\n",
    "                             \n",
    "    return vectorized\n",
    "\n",
    "def extract_label(path):\n",
    "    \n",
    "    splitted_list = re.split('/', path)\n",
    "    \n",
    "    return splitted_list[4]\n",
    "\n",
    "# UDFs\n",
    "\n",
    "feature_extract_UDF = udf(lambda x: feature_extract(x), ArrayType(DoubleType()))\n",
    "\n",
    "to_vector_UDF = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "\n",
    "label_extract_UDF = udf(lambda path: extract_label(path), StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca09e0",
   "metadata": {},
   "source": [
    "<h1>Monitoring Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a47b81",
   "metadata": {},
   "source": [
    "One have to run manualy as many time this script as it wants to try different number of workers e.g : local[1] local[2] etc.\n",
    "Indeed, none of the following option spark.sparkContext.cancelAllJobs(), spark.stop(), spark.sparkContext.stop(), time.sleep(60) would allow to loop different number of workers in a single loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8755bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 4\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder\\\n",
    "        .appName(\"ETL Pipeline\")\\\n",
    "        .master(\"local[{}]\".format(workers))\\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", credentials.access_key)\\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", credentials.secret_key)\\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685acd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.DataFrame(columns=['workers', 'images_size', 'dimentionality','time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6c93d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for image_size in [100,200,300,500,1000]:\n",
    "\n",
    "    s3_object_path = 's3a://oc-projet-8-monitoring/Monitoring/{}/**'.format(image_size)\n",
    "\n",
    "    print(image_size)\n",
    "    \n",
    "    df = spark.read.format(\"binaryFile\").load(s3_object_path)\n",
    "\n",
    "    feature_extracted_df = df.select(feature_extract_UDF(df.content).alias(\"features\"))\n",
    "\n",
    "    vector_df = feature_extracted_df.select(to_vector_UDF('features').alias('features'))\n",
    "\n",
    "    n_components = 100\n",
    "\n",
    "    std = StandardScaler(inputCol=\"features\", outputCol=\"scaled\")\n",
    "    pca = PCA(inputCol=\"scaled\", outputCol=\"pca\").setK(n_components)\n",
    "    pca_only = PCA(inputCol=\"features\", outputCol=\"pca\").setK(n_components)\n",
    "\n",
    "    for stages_select in [[pca_only], [std, pca]]:\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "        pipeline = Pipeline().setStages(stages_select)\n",
    "        \n",
    "        start = datetime.now()\n",
    "\n",
    "        pca_df = pipeline.fit(vector_df).transform(vector_df)\n",
    "\n",
    "        pca_df_multiple_columns = pca_df.withColumn(\"pc\", vector_to_array(\"pca\"))\\\n",
    "                                          .select([col(\"pc\")[i] for i in range(n_components)])\n",
    "\n",
    "        label_df = df.select(label_extract_UDF(df.path).alias(\"labels\"))\n",
    "\n",
    "        pca_df_multiple_columns_id = \\\n",
    "            pca_df_multiple_columns.withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "        label_df_id = label_df.withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "        labeled_pca_df = label_df_id.join(pca_df_multiple_columns_id, on=['id'], how='inner').drop('id')\n",
    "\n",
    "        labeled_pca_df.coalesce(1)\\\n",
    "                   .write.mode('overwrite')\\\n",
    "                   .save(\"s3a://oc-project-8-pca-csv/monitoring\", format='csv', header=True)\n",
    "        \n",
    "        end = datetime.now()\n",
    "        \n",
    "        total_time = end - start\n",
    "        \n",
    "        log_df.loc[i, ['workers', 'images_size', 'dimentionality','time']] = \\\n",
    "            workers, image_size, stages_select, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2977e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
